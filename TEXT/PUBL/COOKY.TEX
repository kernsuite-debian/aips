%-----------------------------------------------------------------------
%;  Copyright (C) 1995
%;  Associated Universities, Inc. Washington DC, USA.
%;
%;  This program is free software; you can redistribute it and/or
%;  modify it under the terms of the GNU General Public License as
%;  published by the Free Software Foundation; either version 2 of
%;  the License, or (at your option) any later version.
%;
%;  This program is distributed in the hope that it will be useful,
%;  but WITHOUT ANY WARRANTY; without even the implied warranty of
%;  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%;  GNU General Public License for more details.
%;
%;  You should have received a copy of the GNU General Public
%;  License along with this program; if not, write to the Free
%;  Software Foundation, Inc., 675 Massachusetts Ave, Cambridge,
%;  MA 02139, USA.
%;
%;  Correspondence concerning AIPS should be addressed as follows:
%;          Internet email: aipsmail@nrao.edu.
%;          Postal address: AIPS Project Office
%;                          National Radio Astronomy Observatory
%;                          520 Edgemont Road
%;                          Charlottesville, VA 22903-2475 USA
%-----------------------------------------------------------------------
\input COOK82.MAC
% Needed to get literal equations correct.
\def\text#1{\hbox{\it#1}}
\def\chapt{Y}
\def\titlea{15-Jul-1995}

\def\Chapt{45}
\tsect{Appendix Y.  FILE SIZES}
\pd

Your data reduction strategy will be more effective if you have an
idea of how big the data and map file sizes will be on disk.  Also, it
will help you estimate just how many files you can \indx{backup} to
tape.  In this appendix, we will discuss file sizes in bytes, which
are 8 bits in size, rather than ``blocks,'' which can vary in size
between different computers.  Inside \AIPS, the definition of ``byte''
is perverted, but all systems now use a 1024-byte (8192 bit)
``block.''

\subsect{Y.1.  Visibility (\uv) data sets}

     \uv\ data files contain information about the coherence function
of a wavefront at random locations and random times.  Consequently,
the way this information is stored on disk is different from that for
images where the pixels are on a regular grid.  AIPS \uv\ data are
stored on disk in a manner similar to the way that it would be
organized on a FITS ``random group'' tape.  The data are stored as
logical records; each record (a ``visibility'') contains {\bf all} the
data taken on one baseline at a given time.  Consequently, a record
may contain information for several IFs, several frequencies at each
of those IFs and more than one polarization combination for each
frequency/IF.  The first part of each logical record contains what are
known as ``random parameters'' \eg\ spatial frequency coordinates and
time.  After the random parameters, there is a small, regular array of
data.

    For a multi-source data set such as might be created by {\tt
\tndx{FILLM}}, the random parameter group will include the following.
{\tt UU-L-SIN}, {\tt VV-L-SIN}, and {\tt WW-L-SIN} give the spatial
frequency coordinates, computed with a sine projection in units of
wavelengths at the reference frequency.  {\tt TIME1} is the label for
the time in days..  {\tt BASELINE} is the baseline number ($256 ant_1
+ ant_2 + subarray/100.$) and {\tt SOURCE} is the source number
corresponding to an entry in the source table.  If you have frequency table
identifiers (which is usually the case these days), then there will be
an additional random parameter, {\tt FQSEL}.  For a compressed
database, two additional random parameters will be required --- {\tt
WEIGHT} to give a single data weight for all samples in the record and
{\tt SCALE} to give the gain used to compress that record.

   The regular data array is similar to an image array in that the
order of axes is arbitrary.  However, the convention is for the first
axis to be of type {\tt COMPLEX}, having a dimension of 3 for
uncompressed data (real, imaginary, weight) and a dimension of 1 for
compressed data.  The other axes of the regular array are {\tt RA},
{\tt DEC}, {\tt FREQ} and {\tt STOKES}.

\subsubsect{Y.1.1.  \uv\ database sizes}

The size of the database to be loaded to disk with {\tt FILLM} is
given by
$$
 \eqalignno{ \Bigr\lbrack \lbrace (\#~random~parameters)
           + \lbrack (dimension~of~\hbox{{\tt COMPLEX}}~axis)
                            \times (\#~polns)
                            &\times (\#~freqs)
                             \times (\#~IFs) \rbrack \rbrace  \cr
                            & \times (\#~vis)
                              \times 4 \Bigr\rbrack
                                        \rm\,bytes }
$$
The number of visibilities is given (approximately) by
$$
   {length~of~observation \over integration~time} \times
                                          number~of~baselines
$$
where the number of baselines is the usual ${1\over2}n(n-1)$.  This
is equal to 351 for the VLA for the usual 27-antenna array.

     For example, a 12-hour observation with 30 second integrations,
one frequency and 2 IFs with RR, RL, LR and LL written in compressed
format ({\tt DOUVCOMP = TRUE} in {\tt \tndx{FILLM}}) will occupy about
34 Mbytes on disk.  In practice, the \uv\ file will usually be a
little larger due to the way the system allocates space on the disks.
You must also remember to allow room for the extension tables --- see
\Sec Y.3.   If this database had been written in uncompressed format,
the \uv\ data would have occupied around 62~Mbyte.

Consider another example illustrated by the {\tt IMHEAD} listing below:
\verbatim{50pt}
Image=MULTI     (UV)         Filename=20/07/90    .L BAND.   1
Telescope=VLA                Receiver=VLA
Observer=AFTST               User #= 1364
Observ. date=20-JUL-1990     Map date=23-JUL-1990
# visibilities    105198     Sort order  TB
Rand axes: UU-L-SIN  VV-L-SIN  WW-L-SIN  BASELINE  TIME1
           SOURCE  FREQSEL  WEIGHT  SCALE
--------------------------------------------------------------
Type    Pixels   Coord value  at Pixel    Coord incr   Rotat
COMPLEX      1   1.0000000E+00    1.00 1.0000000E+00    0.00
STOKES       4  -1.0000000E+00    1.00-1.0000000E+00    0.00
IF           2   1.0000000E+00    1.00 1.0000000E+00    0.00
FREQ         1   1.4524000E+09    1.00 2.5000000E+07    0.00
RA           1    00 00 00.000    1.00      3600.000    0.00
DEC          1    00 00 00.000    1.00      3600.000    0.00
--------------------------------------------------------------
Maximum version number of extension files of type HI is   1
Maximum version number of extension files of type AN is   1
Maximum version number of extension files of type NX is   1
Maximum version number of extension files of type SU is   1
Maximum version number of extension files of type FQ is   1
Maximum version number of extension files of type CL is   1
Maximum version number of extension files of type SN is   2
|endverbatim
\dispe{This compressed ({\tt COMPLEX Pixels = 1}) \uv\ database
contains 9 random parameters, 4 polarizations, 1 frequency, and 2 IFs.
for {\it each} of 105198 visibilities.  The size of the database file
itself is, therefore,
$$
  \Bigl\lbrack \lbrace 9 + \lbrack 1 \times 4 \times 1 \times 2 \rbrack
         \rbrace \times 105198 \times 4 \Bigr\rbrack \rm\,bytes
   = 7.153 \rm\,Mbytes.
$$}

\subsubsect{Y.1.2.   Compressed format for \uv\ data}

     The use of ``compressed'' data can make substantial savings in
the amount of disk space that you require, particularly for
spectral-line databases.  All tasks should now be able to handle
either the compressed or the uncompressed formats.  Compressed data
files can be identified by the dimension of 1 for the {\tt COMPLEX}
axis in the database header.  (Uncompressed data will have a dimension
of 3.)  The savings can be close to a factor of three for spectral
line observations. \iodx{compressed format}

     This is achieved by converting all data weights into a single
{\tt WEIGHT} random parameter, by finding a single {\tt SCALE} random
parameter with which to scale all real and imaginary parts of the
visibilities into 16-bit integers, and by packing the real and
imaginary terms into one 32-bit location using magic-value blanking
for flagged data.  This is to be compared with the uncompressed format
in which each of the real, imaginary and weight terms are stored in a
32-bit floating-point location.

     In general, data compression is a good thing and should be used,
but with a little caution.  With a single frequency, single IF, and
single polarization, you will not save any disk space.  In all other
cases, there are respectable savings to be made.  However, the use of
a packed data word for the real and imaginary parts of the visibility
function along with magic value blanking imposes a restriction on the
``spectral dynamic range'' of the data set of around 32000:1.
Consequently, there are some situations where compressed data should
{\it not} be used.  For example, if the spectral dynamic range in the
\uv\ database is likely to be greater than, say, 1000:1, you must use
{\it uncompressed} data format to avoid loss of accuracy.  This
situation can arise in maser spectra, for example, in which there are
maser lines of 1~Jy and $ >32000$~Jy; in this case, you should never
use compressed data.  Bandpass calibration can cause large correction
factors to be applied to the edge channels of a database.  In the
presence of noise or interference, bad channels can become very much
greater in amplitude than good channels.  In such cases you must
either use uncompressed format or be very careful to flag bad channels
or to drop them with the {\tt BCHAN} and {\tt ECHAN} adverbs {\it as}
you apply the bandpass calibration.  In general, continuum data sets
should be loaded with data compression since these dynamic range
considerations will not normally apply.\iodx{compressed format}

\subsect{Y.2.  Image files}

     Since images are regular arrays, the sizes of image files are
easier to calculate.  The images are stored as floating-point numbers,
\ie\ 32 bits per pixel, so the image file size in bytes is given by
$$
     4 \times \prod_{i} length(i)
$$
where $length(i)$ is the number of pixels on the $i^{th}$ axis.  For
example, a 128-channel cube with each plane a $256 \times 256$ image
will require around 34 Mbytes of disk storage space.  This may be
increased a little due to the way in which the system allocates space
for files.

\subsect{Y.3.  Extension files}

     Subsidiary data about \uv\ database and image files are written
in ``\indx{extension files}.''  These include, for example, history
records ({\tt HI} files), plot instructions ({\tt PL} files),
calibration solutions ({\tt SN} files), ad nauseum.  Some extension
files can become large.  A history file uses 1024 bytes for every 14
history records, a small amount under most circumstances.  A plot file
is normally small, but the output from {\tt \tndx{GREYS}} can be as
large as one plane of the image and the output of {\tt \tndx{KNTR}}
is larger by a factor of the number of panes in the plot.\iodx{plot
file}\iodx{history file}

     The {\tt CL} (calibration) table contains the total model of the
interferometer at each interval.\iodx{calibration file}  Many of these
logical records are blank in the case of most interferometers but, for
the VLBA, these records will contain essential information.  The {\tt
CL} table contains a logical record for each antenna in the array at
each \tndx{CL} time stamp. The \tndx{CL} time stamps are set by the
user when loading the data.  The default for VLA data is every~5
minutes.  For VLBI data, it is every 1~minute.  Each {\tt CL} logical
record requires
$$
  \lbrace 15 + \lbrack 14 \times  (\#~IFs) \times
       (\#~pols)\rbrack\rbrace \times 4 \rm\, bytes
$$
For a 12-hour observation with the VLA with 2 IFs and 4 polarization
pairs, with entries every 5 minutes, the CL table will occupy about
$$
   \Bigl\lbrack\lbrace 15 + \lbrack 14 \times 2 \times 4 \rbrack\rbrace
   \times 4 \times {(12 \times 60) \over 5 } \times 27 \Bigr\rbrack
   \rm\, bytes
   = 1.975 \rm\, Mbytes
$$

     Since most other files are considerably smaller, their sizes can
be ignored.  That they exist and may require some disk, should not be
forgotten.  To look at the full \AIPS\ disk usage on your computer in
summary form:
\dispt{TASK\qs'\tndx{DISKU}' ; INP \CR}{to review the inputs.}
\dispt{INDISK\qs 0 ; DETIME\qs 0 \CR}{to look at all disks and all
               data sets}
\dispt{USERID\qs 32000 \CR}{to look at all user numbers.}
\dispt{DOALL\qs FALSE ; GO \CR}{to run the task in a summary mode.}
\dispe{Then to look at file sizes in detail:}
\dispt{INDISK\qs$n$ ; USERID\qs 0 \CR}{to restrict the display to your
                data sets and one disk.}
\dispt{DOALL\qs TRUE ; GO \CR}{to run the task to list all files on
                disk $n$.}
\pd

\subsect{Y.4.  Storing data on tape}

     Images and \uv\ databases are written to \indx{magnetic tape} by
{\tt \tndx{FITTP}} for archival purposes and for transfer to other
computers and sites.  Three FITS-standard formats are available,
controlled through the adverb {\tt FORMAT}.  The preferred format is
32-bit floating point (IEEE standard) format.  There are no dynamic
range limitations in this format and, on many modern computers, no bit
manipulation is required since they use IEEE floating internally.
\iodx{tape}\iodx{backup}

     Of the two integer formats, there is no reason whatsoever to use
the 32-bit integer since it poses dynamic range, rescaling, and other
problems with no saving in space.  The 16-bit integer format uses
16-bit signed 2's complement integers to represent the data.  Such
numbers are limited to the range -32768 to 32767.  {\tt FITTP} has to
find the maximum and minimum in the image or the uv data set
(including the random parameters) and then scale the data to fit in
this numeric range.  For images of limited dynamic range, this format
is perfectly adequate.  For high-dynamic range images and for \uv\
data, the 16-bit format may not be adequate.  (More than one user has
reduced all his ``good'' spectral channels to pure 0 by scaling all
the \uv\ data to include one really horrendously bad sample.)
A less important benefit of the floating point format is that the
numbers representing your data are recorded exactly on tape as they
are stored on disk; there are no ``quantization errors''.  This may be
important for software development.

     The preceding paragraphs do not tell the full story, however.
The FITS standard does not allow for \uv\ data on tape in a
\indx{compressed format}. Instead, {\tt \tndx{FITTP}} expands the data
into the uncompressed form and then writes the data on tape (with
scaling for the integer formats, if required).  In the conversion, the
real and imaginary values that were stored in one packed number are
expanded into three real values --- one each for real, imaginary and
weight terms --- and the weight and scale random parameters are
removed since they are no longer required.  If the data are written in
16 bit format, the {\tt TIME1} random parameter will be stored in {\bf
two} random parameters to maintain the required precision.
Consequently, the compressed data are expanded to
$$
    \lbrace (\#~random~parameters - 1 ) + \lbrack (\#~pol) \times
        (\#~IFs) \times (\#~frequencies) \times 3 \rbrack \rbrace
      \over
     \lbrace (\#~random~parameters) + \lbrack (\#~pol) \times (\#~IFs)
       \times (\#~frequencies) \rbrack \rbrace
$$
the original size  (where $\#~random~parameters$ is the original
number in the compressed database).  After the expansion, the data are
scaled for the integer formats, if required.  For the 16-bit format,
the size of the database on tape is half of the expanded size since
the 32-bit format on disk is mapped to 16 bits on tape.  (For the 32
bit formats, the number of random parameters in the expanded database
is 2 less than were in the original database.)

     As an example, let us consider a multi-source spectral-line
database stored on disk in \indx{compressed format}.  The data set has
seven channels each at 2 IFs with 2 polarizations.  There are nine
random parameters and 834031 visibilities.  From \Sec Y.1, we can
calculate the size of the \uv\ file to be 123~Mbytes.  (Remember, this
doesn't include any  of the extension files, some of which might be
several Mbytes in size.)   Before the file is written to tape in
16-bit integer format, it is first expanded by a factor of
$$
 {\lbrace (9 - 1) + \lbrack 2 \times 2 \times
                                      7 \times 3 \rbrack \rbrace
\over
\lbrace 9 + \lbrack 2 \times 2 \times 7 \rbrack \rbrace}  = 2.486
$$
after which it is effectively reduced by a factor of two by the
conversion to the 16-bit format.  Consequently, the data will occupy
$$
   {{123 \times 2.486}  \over 2} \rm\, Mbytes
     = 153\rm\, Mbytes
$$
on tape.  In other words, this database and all the associated
extension files are likely to fit on one standard, 6250~bpi tape using
{\tt \tndx{BLOCKING} = 10} (\Sec Y.4.1) and {\tt FORMAT = 1}.
\iodx{tape}\iodx{backup}\iodx{magnetic tape}

     Note that {\tt FITTP} write history file data into the FITS
header and writes table extension files as extensions after the main
image or data set within the same tape file.  Plot ({\tt PL}) and
slice ({\tt SL}) files are not saved to tape.\iodx{plot file}

\subsubsect{Y.4.1. 9-track tapes}

     On {\it \indx{9-track tape}} devices the data are recorded on
eight tracks and the ninth track is used to record a parity bit for
error checking.  Standard recording densities are 800, 1600, and 6250
bits per inch per track ({\it \indx{bpi}}).  Generally, data are
recorded at 6250 bpi wherever possible.  Each tape has a reflective
metallic marker near the start of the tape to signify {\it
beginning-of-tape} or \hbox{BOT}.  The BOT marker is used to allow the
tape drive to position the tape at the starting position. Similarly,
an EOT ({\it end-of-tape}) maker is located near the physical end of
the tape. This is sensed by the tape drive and control circuitry
prevents the device spooling the tape completely off the main spool;
this is very useful for self-loading tape drives!

     Data are written on 9-track tapes in user-controlled record
lengths separated by ``inter-record gaps.''  The lengths of the
inter-record gaps are roughly $3\over5$ of an inch at recording
densities of 800 and 1600 bpi and $3\over10$ of an inch at 6250 bpi.
The standard 9-track tape length is 2400 feet.  However, by the use of
a thinner tape substrate for the oxide layer (made of Mylar), the
regular spool can accommodate up to 3600 feet of tape.

     FITS files are written in fixed blocks of length $n \times 2880$
bytes, where $1 \leq n \leq 10$ by international convention.  The
adverb {\tt \tndx{BLOCKING} = $n$} lets you control the block sizes
written by \hbox{{\tt \tndx{FITTP}}}.   The {\it tape blocking
efficiency} is a measure of the amount of tape which is not simply
``wasted'' space.  It is defined as
$$
   { {block~size \over recording~density} \over
      { \lbrack \lbrace {block~size \over recording~density} \rbrace
      + \text{\it{length~of~inter-record~gap}}} \rbrack }
$$
This gives a tape blocking efficiency of around 0.61 for a 6250-bpi
tape using 2880 byte blocks.  For the maximum block size (10), the
records are 28800 bytes long and the blocking efficiency is around
0.95 at 6250 bpi.

     In round numbers, the capacity of a standard 2400 feet tape is
therefore about 110 Mbytes using {\tt BLOCKING = 1} and around 170
Mbyte using {\tt BLOCKING = 10}.  As an example: images are stored in
floating point as 4 bytes/pixel.  This means that a typical image of
$512 \times 512$ pixels will occupy around 1 Mbyte on disk. Backing up
such images to tape in 32-bit IEEE floating-point ({\tt FORMAT = 3})
with {\tt BLOCKING = 10} will allow you to fit over 150 such images to
a standard 2400-foot tape at 6250 bpi.\iodx{9-track tape}
\iodx{tape}\iodx{backup}\iodx{magnetic tape}

\subsubsect{Y.4.2. DAT and Exabyte tapes}

     The arrival of modern tape technologies has hastened the demise
of 9-track tapes.  First \indx{Exabyte (8mm)} and then \indx{DAT
(4mm)} have provided much higher storage capacities than the 9-track
tapes and have also provided faster seeks between file marks and
greater data reliability.  The new technologies are very much cheaper
as well, in part because they have been adopted by the PC market.
They are both technically quite complex internally.  The DAT tape has
a ``system log'' area at the beginning which allows for the fast
seeks.  It is a bit fragile, however, since it is updated when the
tape is unloaded and hence can be incorrect if there is an unfortunate
power failure.  Both technologies are still evolving and both now
offer various data encoding/compression options.  Unfortunately, the
data compression techniques vary considerably with tape model and
manufacturer and hence should not be used to archive or transport
data.  The data are blocked on the tapes by means known only to the
manufacturers and are not significantly under user control.  It is
still probably good to use a large {\tt \tndx{BLOCKING}}, but only for
I/O transfer reasons.  The EOF marks can be expensive on these tape
devices.

      Exabytes at low density have a capacity of about 2.2 Gbytes on a
112m tape and use about 1 Mbyte (or maybe even 4 Mbytes) for each EOF
mark.  The large size of the EOF limits the number of files you can
write rather significantly.  The EOFs are also slow to process
mechanically.  Exabytes at high density have a capacity of 4.5 Gbytes
on a 112m tape and use 48 Kbytes per EOF mark.  DATs have a capacity
of 2.0 Gbytes on a 90m tape, but also come in 60m and 120m sizes.  The
EOF mark size is not readily available, but is probably no more than
48 Kbytes.  The early warning of the end-of-medium is 40 Mbytes before
the actual end of tape.

\subsect{Y.5.  Very large data sets}

     {\tt \tndx{FITTP}} cannot write multi-volume tapes.  Some
spectral-line and VLBA databases (and perhaps some continuum
databases) may be so large that the file cannot fit on one tape even
with {\tt FORMAT = 1} and {\tt BLOCKING = 10}.  What can you do to
\indx{backup} your data?  If you must use 9-track tape, or the data
set is so large that it will not fit on a DAT or Exabyte, there are
several approaches that you can adopt.

     First, you could {\tt \tndx{SPLIT}} out the database into {\it
single-source} databases and back each of these up individually.
Alternatively, you could subdivide the large database in several
smaller databases with {\tt \tndx{UVCOP}} by specifying a different
time range for each of the smaller databases and then back these up
individually.  Another way to solve the problem is to realize that the
calibration and flagging information that you have carefully generated
during the calibration is contained in the extension tables   --- the
raw data that you loaded is not modified until you finally {\tt SPLIT}
out the individual sources.  Consequently, you can write create a
dummy \uv\ database to which all the extension tables are attached
with the task {\tt \tndx{TASAV}}, then save this ``database'' on tape
with \hbox{{\tt FITTP}}.  The raw visibilities can be saved in the
form of copies of the archive tapes.


\end
